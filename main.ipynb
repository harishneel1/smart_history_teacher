{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "# Adjust the path to where your PDF is located\n",
    "data_path = \"./books\"\n",
    "\n",
    "# Load all PDFs from the directory\n",
    "loader = DirectoryLoader(\n",
    "    data_path,\n",
    "    glob='*.pdf',\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "    \n",
    "# This gives you one document per page\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 480 pages from PDF(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(documents)} pages from PDF(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_documents = text_splitter.split_documents(documents)\n",
    "len(chunk_documents) \n",
    "\n",
    "# So now we have a list of documents with much smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"sapiens\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Now you can add these to your vector store\n",
    "doc_ids = [str(uuid.uuid4()) for _ in range(len(chunk_documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/29\n",
      "Processed batch 2/29\n",
      "Processed batch 3/29\n",
      "Processed batch 4/29\n",
      "Processed batch 5/29\n",
      "Processed batch 6/29\n",
      "Processed batch 7/29\n",
      "Processed batch 8/29\n",
      "Processed batch 9/29\n",
      "Processed batch 10/29\n",
      "Processed batch 11/29\n",
      "Processed batch 12/29\n",
      "Processed batch 13/29\n",
      "Processed batch 14/29\n",
      "Processed batch 15/29\n",
      "Processed batch 16/29\n",
      "Processed batch 17/29\n",
      "Processed batch 18/29\n",
      "Processed batch 19/29\n",
      "Processed batch 20/29\n",
      "Processed batch 21/29\n",
      "Processed batch 22/29\n",
      "Processed batch 23/29\n",
      "Processed batch 24/29\n",
      "Processed batch 25/29\n",
      "Processed batch 26/29\n",
      "Processed batch 27/29\n",
      "Processed batch 28/29\n",
      "Processed batch 29/29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a reasonable batch size\n",
    "batch_size = 50  # Start small and adjust as needed\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, len(chunk_documents), batch_size):\n",
    "    # Get current batch of documents and IDs\n",
    "    batch_docs = chunk_documents[i:i+batch_size]\n",
    "    batch_ids = doc_ids[i:i+batch_size]\n",
    "    \n",
    "    # Add the batch to the vector store\n",
    "    vector_store.add_documents(documents=batch_docs, ids=batch_ids)\n",
    "    \n",
    "    print(f\"Processed batch {i//batch_size + 1}/{(len(chunk_documents) + batch_size - 1)//batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query):\n",
    "    documents = vector_store.similarity_search(query=query)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# Create the chain properly\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda query: get_context(query),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt_template \n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
